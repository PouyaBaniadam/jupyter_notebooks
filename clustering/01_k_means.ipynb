{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5859419721f7d4",
   "metadata": {},
   "source": [
    "## K-Means algorithm\n",
    "\n",
    "K-Means is the most popular and straightforward algorithm for clustering. It is a **centroid-based** algorithm, meaning it relies on finding the center points of the clusters.\n",
    "\n",
    "The algorithm is iterative, following a few simple steps:\n",
    "\n",
    "### The K-Means Steps\n",
    "\n",
    "1.  **Decide on K:** We first decide on the value of **K**, which is the total number of clusters (groups) we want to find. (*For example, K=3*).\n",
    "\n",
    "2.  **Initialize Centroids:** The algorithm randomly chooses $K$ points in the dataset to act as the initial center points, or **Centroids**.\n",
    "    > ![](assets/kmeans_step1_initial.png)\n",
    "\n",
    "3.  **Assign Points (The Assignment Step):** The algorithm assigns every single data point to the cluster of the **closest Centroid**. This \"closeness\" is calculated using the distance formula (like Euclidean distance).\n",
    "\n",
    "    > ![](assets/kmeans_step2_assignment.png)\n",
    "\n",
    "4.  **Update Centroids:** After all points are assigned, the algorithm moves the Centroid of each cluster to the true center (the **mean position**) of all the points currently assigned to it.\n",
    "\n",
    "5.  **Iterate:** Steps 3 and 4 repeat until the centroids stop moving significantly. This indicates the clusters have stabilized, and the algorithm has **converged**.\n",
    "\n",
    "    > ![](assets/kmeans_step3_converged.png)\n",
    "\n",
    "### The Objective Function (Minimizing Error)\n",
    "\n",
    "The entire K-Means process is an attempt to make the clusters as \"tight\" as possible by minimizing the **Sum of Squared Errors (SSE)**.\n",
    "\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^{n} (x_i - c_j)^2\n",
    "$$\n",
    "\n",
    "This formula calculates the total squared distance between every data point ($x_i$) and the center ($c_j$) of its cluster. The algorithm keeps running until this total error value is minimized.\n",
    "\n",
    "### Important Notes on K-Means\n",
    "\n",
    "K-Means is fast, but it has a crucial limitation:\n",
    "\n",
    "*   **Local Optimum:** Because the initial centroids are chosen randomly, K-Means does not guarantee that it will find the absolute best grouping (the global optimum). It might only find a **local optimum**—a good solution for that specific random start.\n",
    "\n",
    "**The Fix:** The standard practice is to **run the K-Means algorithm many times** (e.g., 20 or 50 times) with different random starting points and then choose the final clustering result that has the **lowest overall SSE**.\n",
    "\n",
    "---\n",
    "\n",
    "## Find the best value for the variable \"**_K_**\"\n",
    "\n",
    "Ok! We know how the system works, but how can we find the **_\"K\"_**? I mean we can give a number like 3 or 4 or ... anything, but it does not really make sense to give at randomly. Well, don't worry; we have a solution for that.\n",
    "\n",
    "As you know if we have _n_ instances, we can have from one up to _n_ clusters. But if we think logically, we cannot have _n_ clusters! Otherwise, what is the point of clustering at all?! From the other hand, we cannot have one cluster too! So we have to discuss the best **_\"K\"_** for this.\n",
    "\n",
    "The first solution is to test the algorithm with different **_\"K\"s_**. Each that has better results and less error, has a higher chance of being the best model out there.\n",
    "\n",
    "The other solution is to draw a plot. The image below is called the \"Elbow method\"\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/elbow_method.png)\n",
    "\n",
    "---\n",
    "\n",
    "As seen in the image, the **_\"K\"_** value from 1 to 3 is getting a good decreasing **mean distance**. But right from **_K_** = **_4_**, the incline is getting lower. In this case, we say that **_K_** = **_3_** seems to be the best **_\"K\"_** out there!\n",
    "\n",
    "---\n",
    "\n",
    "Enough theory! Let's do a quick lab here to see what's going on in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3be7e7d8739ef",
   "metadata": {},
   "source": [
    "# Diving into code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a28747d1c2cd",
   "metadata": {},
   "source": [
    "**_Note that_** The following dataset is coming from IBM\n",
    "\n",
    "Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data.\n",
    "Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retain those customers. Another group might include customers from non-profit organizations and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897aa294c87a1791",
   "metadata": {},
   "source": [
    "### Load Data From CSV File\n",
    "\n",
    "Before working with our data, let's load the customers segmentation csv file and look at the main structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "f2730e2c53def1e0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "cust_df = pd.read_csv(\"data/customer_segmentation.csv\")\n",
    "cust_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5906b453d665d7b2",
   "metadata": {},
   "source": [
    "## Pre processing data\n",
    "\n",
    "We actually don't need \"**_Address_**\" in here because it does not matter at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "84019125e2aff950",
   "metadata": {},
   "source": [
    "df = cust_df.drop(columns=[\"Address\"])\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a23053221e82ff9",
   "metadata": {},
   "source": [
    "### Let's normalize our data a little bit.\n",
    "\n",
    "Why?\n",
    "\n",
    "Well Imagine you have a dataset about customers with two features:\n",
    "\n",
    "*   **Age:** (e.g., from 20 to 70)\n",
    "*   **Income:** (e.g., from 30,000 to 150,000)\n",
    "\n",
    "Now, imagine you are the K-Means algorithm trying to calculate the distance between two customers. A difference of **10** in age (e.g., 30 vs 40) is quite significant. But a difference of **10** in income (e.g., \\$50,000 vs \\$50,010) is tiny and meaningless.\n",
    "\n",
    "Because the numbers for **Income** are so much bigger, any distance calculation will be completely dominated by the Income feature. The algorithm will essentially ignore Age, not because Age is unimportant, but because its numerical contribution to the distance is just too small to matter.\n",
    "\n",
    "It's like comparing apples and oranges. The algorithm can't make a fair comparison.\n",
    "\n",
    "### The Solution: StandardScaler\n",
    "\n",
    "`StandardScaler` fixes this problem. For each feature, it does the following:\n",
    "\n",
    "1.  It calculates the average (mean) and the standard deviation.\n",
    "2.  It then transforms each value so that the new average of the feature is **0** and the new standard deviation is **1**.\n",
    "\n",
    "### The Result\n",
    "\n",
    "After using `StandardScaler`, your data might look something like this:\n",
    "\n",
    "| Feature | Original Value | Scaled Value |\n",
    "| :--- | :--- | :--- |\n",
    "| **Age** | 35 | -0.52 |\n",
    "| **Income** | 90,000 | 1.25 |\n",
    "| **Age** | 25 | -1.50 |\n",
    "| **Income** | 45,000 | -0.80 |\n",
    "\n",
    "Now, a change of \"1\" in the scaled Age is just as significant as a change of \"1\" in the scaled Income. All features are now on a level playing field and have an equal chance to influence the clustering result.\n",
    "\n",
    "\n",
    "## A quick example :\n",
    "\n",
    "Imagine we have a small dataset with just one feature: **Age**.\n",
    "\n",
    "**Our Raw Data (Original Ages):**\n",
    "`[25, 30, 35, 40, 50]`\n",
    "\n",
    "`StandardScaler` follows a two-step process to find the \"ingredients\" it needs, and a final step to apply the transformation.\n",
    "\n",
    "### Step 1: Calculate the Mean (the Average)\n",
    "\n",
    "First, we find the average of our data.\n",
    "\n",
    "$$\n",
    "\\text{Mean (μ)} = \\frac{25 + 30 + 35 + 40 + 50}{5} = \\frac{180}{5} = 36\n",
    "$$\n",
    "\n",
    "So, the average age is **36**.\n",
    "\n",
    "### Step 2: Calculate the Standard Deviation (the \"Spread\")\n",
    "\n",
    "Next, we calculate how spread out the data is. This takes a few sub-steps:\n",
    "\n",
    "1.  For each number, subtract the mean (36) and square the result.\n",
    "    *   $(25 - 36)^2 = (-11)^2 = 121$\n",
    "    *   $(30 - 36)^2 = (-6)^2 = 36$\n",
    "    *   $(35 - 36)^2 = (-1)^2 = 1$\n",
    "    *   $(40 - 36)^2 = (4)^2 = 16$\n",
    "    *   $(50 - 36)^2 = (14)^2 = 196$\n",
    "2.  Find the average of these squared results.\n",
    "    *   $\\frac{121 + 36 + 1 + 16 + 196}{5} = \\frac{370}{5} = 74$\n",
    "3.  Take the square root of that average.\n",
    "    *   $\\sqrt{74} \\approx 8.6$\n",
    "\n",
    "So, the standard deviation (σ) is approximately **8.6**.\n",
    "\n",
    "### Step 3: Apply the StandardScaler Transformation\n",
    "\n",
    "Now we have our two \"ingredients\": **Mean (μ) = 36** and **Standard Deviation (σ) = 8.6**.\n",
    "\n",
    "The formula for `StandardScaler` is:\n",
    "\n",
    "$$\n",
    "\\text{Scaled Value} = \\frac{\\text{Original Value} - \\text{Mean}}{\\text{Standard Deviation}}\n",
    "$$\n",
    "\n",
    "Let's apply this to every one of our original age values:\n",
    "\n",
    "| Original Age | Calculation | Scaled Age |\n",
    "| :---: | :---: | :---: |\n",
    "| **25** | (25 - 36) / 8.6 | **-1.28** |\n",
    "| **30** | (30 - 36) / 8.6 | **-0.70** |\n",
    "| **35** | (35 - 36) / 8.6 | **-0.12** |\n",
    "| **40** | (40 - 36) / 8.6 | **0.47** |\n",
    "| **50** | (50 - 36) / 8.6 | **1.63** |\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "We successfully transformed our original data into a new, scaled version:\n",
    "\n",
    "*   **Original Data:** `[25, 30, 35, 40, 50]`\n",
    "*   **Scaled Data:** `[-1.28, -0.70, -0.12, 0.47, 1.63]`\n",
    "\n",
    "This new set of numbers has a **mean of 0** and a **standard deviation of 1**, and it's now ready to be used in a distance-based algorithm like K-Means without any risk of bias due to its scale."
   ]
  },
  {
   "cell_type": "code",
   "id": "fd27fe98acf195a",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "X = df.values[:,1:] # From Customer Id all the way to the DebtIncomeRatio\n",
    "X = np.nan_to_num(X) # NAN values -> 0, infinite -> A very large number\n",
    "Clus_dataSet = StandardScaler().fit_transform(X)\n",
    "Clus_dataSet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28a8524bd45b2d0e",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "\n",
    "In our example (if we didn't have access to the k-means algorithm), it would be the same as guessing that each customer group would have certain age, income, education, etc, with multiple tests and experiments. However, using the K-means clustering we can do all this process much easier.\n",
    "\n",
    "Let's apply k-means on our dataset, and take look at cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "id": "a71ea361ebaf8c0f",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterNum = 3\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 30)\n",
    "k_means.fit(Clus_dataSet)\n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f8693c6e56eee79",
   "metadata": {},
   "source": [
    "df[\"Clustered Label\"] = labels\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2fd628b7aff87ae",
   "metadata": {},
   "source": [
    "overal_view = df.groupby(\"Clustered Label\").mean().drop(columns=[\"Customer Id\"])\n",
    "overal_view"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e1617dac4c93483",
   "metadata": {},
   "source": [
    "### Let's show the data on the plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "608e4a616468d878",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = np.pi * ( X[:, 1])**2\n",
    "plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(float), alpha=0.5)\n",
    "plt.xlabel('Age', fontsize=18)\n",
    "plt.ylabel('Income', fontsize=16)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59dc40b6ca86210b",
   "metadata": {},
   "source": [
    "### Why **_K = 3_** ?\n",
    "\n",
    "Well, as we have discussed before, we have elbow method in K-means algorithm. so let's do the same in here as well to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8287e590791488b",
   "metadata": {},
   "source": [
    "wcss = []\n",
    "k_values = range(1, 16)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is WCSS\n",
    "\n",
    "plt.plot(k_values, wcss, 'bo-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method For Optimal K')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d41db67c3aaebd8",
   "metadata": {},
   "source": [
    "As you can see, the diagram has a lower inclinde decrease at point 3. This confirms that this is a good trade-off bewtween complexity and performance!\n",
    "\n",
    "How ever you might get better results for **_K = 4_** as well. you can test that yourself. why don't you try that out?\n",
    "\n",
    "Anyway, that was all about K-means. Let's dive into the next algorithm called **_Hierarchial_**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
