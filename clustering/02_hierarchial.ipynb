{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f39f821bd258e78",
   "metadata": {},
   "source": [
    "# Hierarchial algorithm\n",
    "\n",
    "Hierarchial algorithm is the technique to organize data in a tree-view shape like the image below:\n",
    "\n",
    "---\n",
    "\n",
    " ![](assets/dendrogram.webp)\n",
    "\n",
    "---\n",
    "\n",
    "The diagram above is called a dendrogram. In this tree-view, every node is a cluster for itself. So we can take advantage of not selecting any random \"**_K_**\" and test which one is better. we can always cut the tree to match our needed clusters count!\n",
    "\n",
    "There are 2 ways to make this tree. one is called **_Divisive_** and the other is called **_Agglomerative_**.\n",
    "\n",
    "The only difference is where we start our tree and continue. In **_Agglomerative_**, we go from down to up and in the **_Divisive_**, we start from the very top side to the bottom of the tree.\n",
    "\n",
    "---\n",
    "\n",
    "## A quick example of how it works\n",
    "\n",
    "Imagine we have a dataset like this. This is the distance of 7 cities in Iran.\n",
    "\n",
    "| | Tehran (TE) | Isfahan (IS) | Shiraz (SH) | Mashhad (MA) | Tabriz (TB) | Yazd (YA) | Ahvaz (AH) |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Tehran (TE)** | 0 | 450 | 930 | 890 | 630 | 620 | 800 |\n",
    "| **Isfahan (IS)** | | 0 | 480 | 1130 | 880 | 320 | 500 |\n",
    "| **Shiraz (SH)** | | | 0 | 1350 | 1360 | 430 | 540 |\n",
    "| **Mashhad (MA)**| | | | 0 | 1500 | 900 | 1550 |\n",
    "| **Tabriz (TB)** | | | | | 0 | 1250 | 1100 |\n",
    "| **Yazd (YA)** | | | | | | 0 | 800 |\n",
    "| **Ahvaz (AH)** | | | | | | | 0 |\n",
    "\n",
    "In this table, what we should do in the Hierarchial algorithm is to find the lowest distance. with a quick check, the lowest distance is between **_Isfahan_** and **_Yazd_**.\n",
    "\n",
    "Then we will create the table again, but this time, we have just combined **_Isfahan_** and **_Yazd_** together.\n",
    "\n",
    "---\n",
    "\n",
    "| | Tehran (TE) | **(IS-YA)** | Shiraz (SH) | Mashhad (MA) | Tabriz (TB) | Ahvaz (AH) |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Tehran (TE)** | 0 | 450 | 930 | 890 | 630 | 800 |\n",
    "| **(IS-YA)** | | 0 | 430 | 900 | 880 | 500 |\n",
    "| **Shiraz (SH)** | | | 0 | 1350 | 1360 | 540 |\n",
    "| **Mashhad (MA)**| | | | 0 | 1500 | 1550 |\n",
    "| **Tabriz (TB)** | | | | | 0 | 1100 |\n",
    "| **Ahvaz (AH)** | | | | | | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "So, now we should check all of the distances again. but the question is how we calculate the distance between **_Tehran_** and **_(IS-YA)_**?\n",
    "\n",
    "Well, there are several ways to do this, known as **\"Linkage Methods.\"** The method used to create the table above is the simplest one, called **Single Linkage**. It defines the distance from a city (like Tehran) to the new cluster `(IS-YA)` as the **minimum** of the distances to the original cities. For example:\n",
    ">\n",
    "> `Distance(Tehran, (IS-YA)) = min(Distance(Tehran, IS), Distance(Tehran, YA))`\n",
    ">\n",
    "> `Distance(Tehran, (IS-YA)) = min(450, 620) = 450`\n",
    "\n",
    "Note that there are 2 more common ways to calculate this distance, which are called **Complete Linkage** and **Average Linkage**. Together, these three are the most fundamental \"Linkage Methods.\"\n",
    "\n",
    "### 1. Complete Linkage (MAX)\n",
    "\n",
    "This method is the exact opposite of Single Linkage. It defines the distance from a city to the new cluster as the **maximum** distance to any of its members. It's more conservative and looks at the furthest possible points to determine the cluster distance.\n",
    "\n",
    "Using our example:\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = max(Distance(Tehran, IS), Distance(Tehran, YA))`\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = max(450, 620) = 620`\n",
    "\n",
    "*   **Impact:** This method tends to produce more compact, spherical clusters and is much less sensitive to outliers than Single Linkage.\n",
    "\n",
    "### 2. Average Linkage (MEAN)\n",
    "\n",
    "This method provides a balance between the other two. It defines the distance from a city to the new cluster as the **average** of the distances to all of its members.\n",
    "\n",
    "Using our example:\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = (Distance(Tehran, IS) + Distance(Tehran, YA)) / 2`\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = (450 + 620) / 2 = 535`\n",
    "\n",
    "*   **Impact:** This method is less affected by outliers than Single Linkage and often provides a good, balanced result.\n",
    "\n",
    "---\n",
    "\n",
    "> \"This process of finding the closest pair and merging them is repeated until only one single cluster, containing all the cities, remains. The entire history of these merges creates a tree-like structure that we can then analyze.\"\n",
    "\n",
    "---\n",
    "\n",
    "This algorithm is quite heavy and may take long runtimes as we are doing the same thing over and over again on each node. so this might cost a fortune of resources on a very large dataset! But from the other hand, the answer is always the same. No need to double check.\n",
    "\n",
    "Enough **theory**! Let's dive into a real world code to see what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490a745feb965",
   "metadata": {},
   "source": [
    "Imagine that an automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n",
    "\n",
    "Our objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46e52745cb6dd2",
   "metadata": {},
   "source": [
    "### Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea4c56ba781ddd4c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'data/cars.csv'\n",
    "\n",
    "pdf = pd.read_csv(filename)\n",
    "print (\"Shape of dataset: \", pdf.shape)\n",
    "\n",
    "pdf.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a4f489234b84366",
   "metadata": {},
   "source": [
    "The feature sets include  price in thousands (price), engine size (engine_s), horsepower (horsepow), wheelbase (wheelbas), width (width), length (length), curb weight (curb_wgt), fuel capacity (fuel_cap) and fuel efficiency (mpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7db1fbecc24df",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Let's clean the dataset by dropping the rows that have null value:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e74850ed9e219073",
   "metadata": {},
   "source": [
    "print (\"Shape of dataset before cleaning: \", pdf.size)\n",
    "pdf[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']] = pdf[['sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\n",
    "pdf = pdf.dropna()\n",
    "pdf = pdf.reset_index(drop=True)\n",
    "print (\"Shape of dataset after cleaning: \", pdf.size)\n",
    "pdf.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d19ba83da81baba9",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Let's select our feature set:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9228fad49f198ca7",
   "metadata": {},
   "source": [
    "featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cc22a10d6050dab",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Now we can normalize the feature set. **MinMaxScaler** transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "id": "e425c07157f75c5d",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = featureset.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "feature_mtx = min_max_scaler.fit_transform(x)\n",
    "feature_mtx [0:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist_matrix = euclidean_distances(feature_mtx,feature_mtx)\n",
    "print(dist_matrix)"
   ],
   "id": "ace551762be09b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')"
   ],
   "id": "91b51f01046d6d08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pylab\n",
    "\n",
    "fig = pylab.figure(figsize=(18,50))\n",
    "def llf(id):\n",
    "    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n",
    "\n",
    "dendro = hierarchy.dendrogram(Z_using_dist_matrix,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"
   ],
   "id": "f4da6640c5cad2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next step, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset.\n",
    "\n",
    "*   Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "*   Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "*   Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n"
   ],
   "id": "3b1157a3a8b5c9e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\n",
    "agglom.fit(dist_matrix)\n",
    "\n",
    "agglom.labels_"
   ],
   "id": "908ba2c82a76106f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pdf['cluster_'] = agglom.labels_\n",
    "pdf.head()"
   ],
   "id": "b8d55870f493e26d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pdf.groupby(['cluster_','type'])['cluster_'].count()",
   "id": "bbcdd2a902bd54df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agg_cars = pdf.groupby(['cluster_','type'])[['horsepow','engine_s','mpg','price']].mean()\n",
    "agg_cars"
   ],
   "id": "bf53ad171679bd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for color, label in zip(colors, cluster_labels):\n",
    "    subset = agg_cars.loc[(label,),]\n",
    "    for i in subset.index:\n",
    "        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n",
    "    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\n",
    "plt.legend()\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('horsepow')\n",
    "plt.ylabel('mpg')"
   ],
   "id": "1e78bd5c8ee35919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That was all about hierarchical. now let's dive into DBSCAN method!",
   "id": "9d7ed44425cae3c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c0b874f563ecd1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
