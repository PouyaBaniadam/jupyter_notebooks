{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff69e3f15b036aa6",
   "metadata": {},
   "source": [
    "# Introduction to Clustering\n",
    "\n",
    "The field of Machine Learning is broadly categorized into **_4_** main approaches, as illustrated below.\n",
    "\n",
    "---\n",
    "\n",
    "$\n",
    "\\text{🤖 Machine Learning}\n",
    "\\begin{cases}\n",
    "  \\text{🧑‍🏫 Supervised Learning} \\\\\n",
    "  \\text{🧩 Unsupervised Learning} \\\\\n",
    "  \\text{⚖️ Semi-supervised Learning} \\\\\n",
    "  \\text{🕹️ Reinforcement Learning}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "This report will focus on **Clustering**, one of the primary tasks within the **'Unsupervised Learning'**.\n",
    "\n",
    "In many machine learning tasks, particularly in **'Supervised Learning'**, the primary goal is a prediction. An algorithm is trained on labeled data to predict an output, such as assigning a label to new, unseen data.\n",
    "\n",
    "From the other hand, **'Clustering'** is not designed to predict a specific output. Instead, its objective is to discover structures within the data by organizing it into meaningful groups, or \"clusters.\"\n",
    "\n",
    "---\n",
    "\n",
    "# A real world example\n",
    "\n",
    "## Bank Example: Finding Risky Customers\n",
    "\n",
    "Banks use clustering to figure out which customers are financially similar and what risk they pose.\n",
    "\n",
    "*   **Goal:** Group customers to manage financial risk.\n",
    "*   **Input:** The bank uses unlabeled financial data like a customers income, debt, and payment history, age and ....\n",
    "*   **Clustering Action:** The algorithm automatically sorts all customers into distinct groups, such as a **\"Low-Risk\"** cluster and a **\"High-Risk\"** cluster.\n",
    "*   **Impact:** The bank can then use these groups to make decisions. For example, they can offer their best interest rates and products to the Low-Risk group while restricting loans or services to the High-Risk group.\n",
    "\n",
    "---\n",
    "\n",
    "## Where to use clustering\n",
    "\n",
    "*   **Exploring data analysis**\n",
    "*   **Summary generation**\n",
    "*   **Outlier detection**\n",
    "*   **Finding duplicates**\n",
    "*   **Pre-processing data**\n",
    "\n",
    "---\n",
    "\n",
    "Now that you know the main idea about clustering, let's dive into the scientiffic parts.\n",
    "\n",
    "For all algorithms in clustering, we actually want to find similarities and dissimilarities.\n",
    "\n",
    "You might wonder why we need dissimilarities! Well the answer is straightforward. As you can see in the image below, we are trying to cluster similar items that are as close as possible together. From the other hand, we are trying to maximize the clusters distances from each other. Look at the image below to see it clearly:\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/intra_inter.png)\n",
    "\n",
    "---\n",
    "\n",
    "As you can see, the yellow items are trying to stick together and stay close, but the green and bule ones are trying to get as far as possible! If we somehowe manage to maximize the **'intra-distances'** and also minimize **'inter-distances'**, we can claim that we are on the right track in clustering.\n",
    "\n",
    "So, the goals are:\n",
    "\n",
    "$$\\text{Dis} (x_1, x_2) \\downarrow$$\n",
    "*(Minimize the distance between points $\\mathbf{x_1}$ and $\\mathbf{x_2}$ in the same cluster)*\n",
    "\n",
    "$$\\text{Dis} (c_1, c_2) \\uparrow$$\n",
    "*(Maximize the distance between clusters $\\mathbf{c_1}$ and $\\mathbf{c_2}$)*\n",
    "\n",
    "---\n",
    "\n",
    "### What is distance and how to measure that?\n",
    "\n",
    "Well, there are many different ways to measure distances between 2 points in vector environment such as :\n",
    "\n",
    "*   **Euclidean**\n",
    "*   **Cosine**\n",
    "*   **Average distance**\n",
    "\n",
    "and ...\n",
    "\n",
    "For now, we are going to talk about '**Euclidean**'\n",
    "\n",
    "In mathematics, the Euclidean distance between two points in Euclidean space is the length of the line segment between them. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, and therefore is occasionally called the Pythagorean distance.\n",
    "\n",
    "You can see it clearly in the image below:\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/euclidean_distance.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "So, it's just simple math. We find the difference of different items like this.\n",
    "\n",
    "$$\n",
    "\\text{Dis}(x_i, x_j) = \\sqrt{\\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}\n",
    "$$\n",
    "\n",
    "You might ask why we add some more steps like changing the numbers to its second power and then used a radical on all of it. Well, we always try to normalize our data with this method, so that we bring our data on the normal curve. It is always a good practice to do that.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Distance Calculation (Euclidean)\n",
    "\n",
    "Imagine we have two data points (e.g., two customers) measured by two features: **Feature A** and **Feature B**.\n",
    "\n",
    "| Feature | Point $x_i$ | Point $x_j$ |\n",
    "| :---: | :---: | :---: |\n",
    "| **Feature A** | 2 | 5 |\n",
    "| **Feature B** | 8 | 4 |\n",
    "\n",
    "To calculate the Euclidean distance between $x_i$ and $x_j$, we follow the steps of the formula:\n",
    "\n",
    "**1. Calculate the difference and square it for each feature:**\n",
    "\n",
    "*   **Feature A:** $(2 - 5)^2 = (-3)^2 = 9$\n",
    "*   **Feature B:** $(8 - 4)^2 = (4)^2 = 16$\n",
    "\n",
    "**2. Sum the squared differences and take the square root:**\n",
    "\n",
    "The calculation for the distance is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Dis}(x_i, x_j) = \\sqrt{(2-5)^2 + (8-4)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Dis}(x_i, x_j) = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "$$\n",
    "\n",
    "The distance between the two points is **5**. This single number is what the clustering algorithm uses to determine how similar $x_i$ and $x_j$ are. A lower distance means higher similarity.\n",
    "\n",
    "---\n",
    "\n",
    "All right, now that we know the main concepts, lets' dive into our very first algorithm, called '**_K-means_**'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5859419721f7d4",
   "metadata": {},
   "source": [
    "## K-Means algorithm\n",
    "\n",
    "K-Means is the most popular and straightforward algorithm for clustering. It is a **centroid-based** algorithm, meaning it relies on finding the center points of the clusters.\n",
    "\n",
    "The algorithm is iterative, following a few simple steps:\n",
    "\n",
    "### The K-Means Steps\n",
    "\n",
    "1.  **Decide on K:** We first decide on the value of **K**, which is the total number of clusters (groups) we want to find. (*For example, K=3*).\n",
    "\n",
    "2.  **Initialize Centroids:** The algorithm randomly chooses $K$ points in the dataset to act as the initial center points, or **Centroids**.\n",
    "    > ![](assets/kmeans_step1_initial.png)\n",
    "\n",
    "3.  **Assign Points (The Assignment Step):** The algorithm assigns every single data point to the cluster of the **closest Centroid**. This \"closeness\" is calculated using the distance formula (like Euclidean distance).\n",
    "\n",
    "    > ![](assets/kmeans_step2_assignment.png)\n",
    "\n",
    "4.  **Update Centroids:** After all points are assigned, the algorithm moves the Centroid of each cluster to the true center (the **mean position**) of all the points currently assigned to it.\n",
    "\n",
    "5.  **Iterate:** Steps 3 and 4 repeat until the centroids stop moving significantly. This indicates the clusters have stabilized, and the algorithm has **converged**.\n",
    "\n",
    "    > ![](assets/kmeans_step3_converged.png)\n",
    "\n",
    "### The Objective Function (Minimizing Error)\n",
    "\n",
    "The entire K-Means process is an attempt to make the clusters as \"tight\" as possible by minimizing the **Sum of Squared Errors (SSE)**.\n",
    "\n",
    "$$\n",
    "\\text{SSE} = \\sum_{i=1}^{n} (x_i - c_j)^2\n",
    "$$\n",
    "\n",
    "This formula calculates the total squared distance between every data point ($x_i$) and the center ($c_j$) of its cluster. The algorithm keeps running until this total error value is minimized.\n",
    "\n",
    "### Important Notes on K-Means\n",
    "\n",
    "K-Means is fast, but it has a crucial limitation:\n",
    "\n",
    "*   **Local Optimum:** Because the initial centroids are chosen randomly, K-Means does not guarantee that it will find the absolute best grouping (the global optimum). It might only find a **local optimum**—a good solution for that specific random start.\n",
    "\n",
    "**The Fix:** The standard practice is to **run the K-Means algorithm many times** (e.g., 20 or 50 times) with different random starting points and then choose the final clustering result that has the **lowest overall SSE**.\n",
    "\n",
    "---\n",
    "\n",
    "## Find the best value for the variable \"**_K_**\"\n",
    "\n",
    "Ok! We know how the system works, but how can we find the **_\"K\"_**? I mean we can give a number like 3 or 4 or ... anything, but it does not really make sense to give at randomly. Well, don't worry; we have a solution for that.\n",
    "\n",
    "As you know if we have _n_ instances, we can have from one up to _n_ clusters. But if we think logically, we cannot have _n_ clusters! Otherwise, what is the point of clustering at all?! From the other hand, we cannot have one cluster too! So we have to discuss the best **_\"K\"_** for this.\n",
    "\n",
    "The first solution is to test the algorithm with different **_\"K\"s_**. Each that has better results and less error, has a higher chance of being the best model out there.\n",
    "\n",
    "The other solution is to draw a plot. The image below is called the \"Elbow method\"\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/elbow_method.png)\n",
    "\n",
    "---\n",
    "\n",
    "As seen in the image, the **_\"K\"_** value from 1 to 3 is getting a good decreasing **mean distance**. But right from **_K_** = **_4_**, the incline is getting lower. In this case, we say that **_K_** = **_3_** seems to be the best **_\"K\"_** out there!\n",
    "\n",
    "---\n",
    "\n",
    "Enough theory! Let's do a quick lab here to see what's going on in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3be7e7d8739ef",
   "metadata": {},
   "source": [
    "# Diving into code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a28747d1c2cd",
   "metadata": {},
   "source": [
    "**_Note that_** The following dataset is coming from IBM\n",
    "\n",
    "Imagine that you have a customer dataset, and you need to apply customer segmentation on this historical data.\n",
    "Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retain those customers. Another group might include customers from non-profit organizations and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897aa294c87a1791",
   "metadata": {},
   "source": [
    "### Load Data From CSV File\n",
    "\n",
    "Before working with our data, let's load the customers segmentation csv file and look at the main structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "f2730e2c53def1e0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "cust_df = pd.read_csv(\"data/customer_segmentation.csv\")\n",
    "cust_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5906b453d665d7b2",
   "metadata": {},
   "source": [
    "## Pre processing data\n",
    "\n",
    "We actually don't need \"**_Address_**\" in here because it does not matter at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "84019125e2aff950",
   "metadata": {},
   "source": [
    "df = cust_df.drop(columns=[\"Address\"])\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a23053221e82ff9",
   "metadata": {},
   "source": [
    "### Let's normalize our data a little bit.\n",
    "\n",
    "Why?\n",
    "\n",
    "Well Imagine you have a dataset about customers with two features:\n",
    "\n",
    "*   **Age:** (e.g., from 20 to 70)\n",
    "*   **Income:** (e.g., from 30,000 to 150,000)\n",
    "\n",
    "Now, imagine you are the K-Means algorithm trying to calculate the distance between two customers. A difference of **10** in age (e.g., 30 vs 40) is quite significant. But a difference of **10** in income (e.g., \\$50,000 vs \\$50,010) is tiny and meaningless.\n",
    "\n",
    "Because the numbers for **Income** are so much bigger, any distance calculation will be completely dominated by the Income feature. The algorithm will essentially ignore Age, not because Age is unimportant, but because its numerical contribution to the distance is just too small to matter.\n",
    "\n",
    "It's like comparing apples and oranges. The algorithm can't make a fair comparison.\n",
    "\n",
    "### The Solution: StandardScaler\n",
    "\n",
    "`StandardScaler` fixes this problem. For each feature, it does the following:\n",
    "\n",
    "1.  It calculates the average (mean) and the standard deviation.\n",
    "2.  It then transforms each value so that the new average of the feature is **0** and the new standard deviation is **1**.\n",
    "\n",
    "### The Result\n",
    "\n",
    "After using `StandardScaler`, your data might look something like this:\n",
    "\n",
    "| Feature | Original Value | Scaled Value |\n",
    "| :--- | :--- | :--- |\n",
    "| **Age** | 35 | -0.52 |\n",
    "| **Income** | 90,000 | 1.25 |\n",
    "| **Age** | 25 | -1.50 |\n",
    "| **Income** | 45,000 | -0.80 |\n",
    "\n",
    "Now, a change of \"1\" in the scaled Age is just as significant as a change of \"1\" in the scaled Income. All features are now on a level playing field and have an equal chance to influence the clustering result.\n",
    "\n",
    "\n",
    "## A quick example :\n",
    "\n",
    "Imagine we have a small dataset with just one feature: **Age**.\n",
    "\n",
    "**Our Raw Data (Original Ages):**\n",
    "`[25, 30, 35, 40, 50]`\n",
    "\n",
    "`StandardScaler` follows a two-step process to find the \"ingredients\" it needs, and a final step to apply the transformation.\n",
    "\n",
    "### Step 1: Calculate the Mean (the Average)\n",
    "\n",
    "First, we find the average of our data.\n",
    "\n",
    "$$\n",
    "\\text{Mean (μ)} = \\frac{25 + 30 + 35 + 40 + 50}{5} = \\frac{180}{5} = 36\n",
    "$$\n",
    "\n",
    "So, the average age is **36**.\n",
    "\n",
    "### Step 2: Calculate the Standard Deviation (the \"Spread\")\n",
    "\n",
    "Next, we calculate how spread out the data is. This takes a few sub-steps:\n",
    "\n",
    "1.  For each number, subtract the mean (36) and square the result.\n",
    "    *   $(25 - 36)^2 = (-11)^2 = 121$\n",
    "    *   $(30 - 36)^2 = (-6)^2 = 36$\n",
    "    *   $(35 - 36)^2 = (-1)^2 = 1$\n",
    "    *   $(40 - 36)^2 = (4)^2 = 16$\n",
    "    *   $(50 - 36)^2 = (14)^2 = 196$\n",
    "2.  Find the average of these squared results.\n",
    "    *   $\\frac{121 + 36 + 1 + 16 + 196}{5} = \\frac{370}{5} = 74$\n",
    "3.  Take the square root of that average.\n",
    "    *   $\\sqrt{74} \\approx 8.6$\n",
    "\n",
    "So, the standard deviation (σ) is approximately **8.6**.\n",
    "\n",
    "### Step 3: Apply the StandardScaler Transformation\n",
    "\n",
    "Now we have our two \"ingredients\": **Mean (μ) = 36** and **Standard Deviation (σ) = 8.6**.\n",
    "\n",
    "The formula for `StandardScaler` is:\n",
    "\n",
    "$$\n",
    "\\text{Scaled Value} = \\frac{\\text{Original Value} - \\text{Mean}}{\\text{Standard Deviation}}\n",
    "$$\n",
    "\n",
    "Let's apply this to every one of our original age values:\n",
    "\n",
    "| Original Age | Calculation | Scaled Age |\n",
    "| :---: | :---: | :---: |\n",
    "| **25** | (25 - 36) / 8.6 | **-1.28** |\n",
    "| **30** | (30 - 36) / 8.6 | **-0.70** |\n",
    "| **35** | (35 - 36) / 8.6 | **-0.12** |\n",
    "| **40** | (40 - 36) / 8.6 | **0.47** |\n",
    "| **50** | (50 - 36) / 8.6 | **1.63** |\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "We successfully transformed our original data into a new, scaled version:\n",
    "\n",
    "*   **Original Data:** `[25, 30, 35, 40, 50]`\n",
    "*   **Scaled Data:** `[-1.28, -0.70, -0.12, 0.47, 1.63]`\n",
    "\n",
    "This new set of numbers has a **mean of 0** and a **standard deviation of 1**, and it's now ready to be used in a distance-based algorithm like K-Means without any risk of bias due to its scale."
   ]
  },
  {
   "cell_type": "code",
   "id": "fd27fe98acf195a",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "X = df.values[:,1:] # From Customer Id all the way to the DebtIncomeRatio\n",
    "X = np.nan_to_num(X) # NAN values -> 0, infinite -> A very large number\n",
    "Clus_dataSet = StandardScaler().fit_transform(X)\n",
    "Clus_dataSet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28a8524bd45b2d0e",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "\n",
    "In our example (if we didn't have access to the k-means algorithm), it would be the same as guessing that each customer group would have certain age, income, education, etc, with multiple tests and experiments. However, using the K-means clustering we can do all this process much easier.\n",
    "\n",
    "Let's apply k-means on our dataset, and take look at cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "id": "a71ea361ebaf8c0f",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterNum = 3\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 30)\n",
    "k_means.fit(Clus_dataSet)\n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f8693c6e56eee79",
   "metadata": {},
   "source": [
    "df[\"Clustered Label\"] = labels\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2fd628b7aff87ae",
   "metadata": {},
   "source": [
    "overal_view = df.groupby(\"Clustered Label\").mean().drop(columns=[\"Customer Id\"])\n",
    "overal_view"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e1617dac4c93483",
   "metadata": {},
   "source": [
    "### Let's show the data on the plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "608e4a616468d878",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = np.pi * ( X[:, 1])**2\n",
    "plt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(float), alpha=0.5)\n",
    "plt.xlabel('Age', fontsize=18)\n",
    "plt.ylabel('Income', fontsize=16)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "59dc40b6ca86210b",
   "metadata": {},
   "source": [
    "### Why **_K = 3_** ?\n",
    "\n",
    "Well, as we have discussed before, we have elbow method in K-means algorithm. so let's do the same in here as well to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8287e590791488b",
   "metadata": {},
   "source": [
    "wcss = []\n",
    "k_values = range(1, 16)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)  # inertia_ is WCSS\n",
    "\n",
    "plt.plot(k_values, wcss, 'bo-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method For Optimal K')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d41db67c3aaebd8",
   "metadata": {},
   "source": [
    "As you can see, the diagram has a lower inclinde decrease at point 3. This confirms that this is a good trade-off bewtween complexity and performance!\n",
    "\n",
    "How ever you might get better results for **_K = 4_** as well. you can test that yourself. why don't you try that out?\n",
    "\n",
    "Anyway, that was all about K-means. Let's dive into the next algorithm called **_Hierarchial_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39f821bd258e78",
   "metadata": {},
   "source": [
    "# Hierarchial algorithm\n",
    "\n",
    "Hierarchial algorithm is the technique to organize data in a tree-view shape like the image below:\n",
    "\n",
    "---\n",
    "\n",
    " ![](assets/dendrogram.webp)\n",
    "\n",
    "---\n",
    "\n",
    "The diagram above is called a dendrogram. In this tree-view, every node is a cluster for itself. So we can take advantage of not selecting any random \"**_K_**\" and test which one is better. we can always cut the tree to match our needed clusters count!\n",
    "\n",
    "There are 2 ways to make this tree. one is called **_Divisive_** and the other is called **_Agglomerative_**.\n",
    "\n",
    "The only difference is where we start our tree and continue. In **_Agglomerative_**, we go from down to up and in the **_Divisive_**, we start from the very top side to the bottom of the tree.\n",
    "\n",
    "---\n",
    "\n",
    "## A quick example of how it works\n",
    "\n",
    "Imagine we have a dataset like this. This is the distance of 7 cities in Iran.\n",
    "\n",
    "| | Tehran (TE) | Isfahan (IS) | Shiraz (SH) | Mashhad (MA) | Tabriz (TB) | Yazd (YA) | Ahvaz (AH) |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Tehran (TE)** | 0 | 450 | 930 | 890 | 630 | 620 | 800 |\n",
    "| **Isfahan (IS)** | | 0 | 480 | 1130 | 880 | 320 | 500 |\n",
    "| **Shiraz (SH)** | | | 0 | 1350 | 1360 | 430 | 540 |\n",
    "| **Mashhad (MA)**| | | | 0 | 1500 | 900 | 1550 |\n",
    "| **Tabriz (TB)** | | | | | 0 | 1250 | 1100 |\n",
    "| **Yazd (YA)** | | | | | | 0 | 800 |\n",
    "| **Ahvaz (AH)** | | | | | | | 0 |\n",
    "\n",
    "In this table, what we should do in the Hierarchial algorithm is to find the lowest distance. with a quick check, the lowest distance is between **_Isfahan_** and **_Yazd_**.\n",
    "\n",
    "Then we will create the table again, but this time, we have just combined **_Isfahan_** and **_Yazd_** together.\n",
    "\n",
    "---\n",
    "\n",
    "| | Tehran (TE) | **(IS-YA)** | Shiraz (SH) | Mashhad (MA) | Tabriz (TB) | Ahvaz (AH) |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| **Tehran (TE)** | 0 | 450 | 930 | 890 | 630 | 800 |\n",
    "| **(IS-YA)** | | 0 | 430 | 900 | 880 | 500 |\n",
    "| **Shiraz (SH)** | | | 0 | 1350 | 1360 | 540 |\n",
    "| **Mashhad (MA)**| | | | 0 | 1500 | 1550 |\n",
    "| **Tabriz (TB)** | | | | | 0 | 1100 |\n",
    "| **Ahvaz (AH)** | | | | | | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "So, now we should check all of the distances again. but the question is how we calculate the distance between **_Tehran_** and **_(IS-YA)_**?\n",
    "\n",
    "Well, there are several ways to do this, known as **\"Linkage Methods.\"** The method used to create the table above is the simplest one, called **Single Linkage**. It defines the distance from a city (like Tehran) to the new cluster `(IS-YA)` as the **minimum** of the distances to the original cities. For example:\n",
    ">\n",
    "> `Distance(Tehran, (IS-YA)) = min(Distance(Tehran, IS), Distance(Tehran, YA))`\n",
    ">\n",
    "> `Distance(Tehran, (IS-YA)) = min(450, 620) = 450`\n",
    "\n",
    "Note that there are 2 more common ways to calculate this distance, which are called **Complete Linkage** and **Average Linkage**. Together, these three are the most fundamental \"Linkage Methods.\"\n",
    "\n",
    "### 1. Complete Linkage (MAX)\n",
    "\n",
    "This method is the exact opposite of Single Linkage. It defines the distance from a city to the new cluster as the **maximum** distance to any of its members. It's more conservative and looks at the furthest possible points to determine the cluster distance.\n",
    "\n",
    "Using our example:\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = max(Distance(Tehran, IS), Distance(Tehran, YA))`\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = max(450, 620) = 620`\n",
    "\n",
    "*   **Impact:** This method tends to produce more compact, spherical clusters and is much less sensitive to outliers than Single Linkage.\n",
    "\n",
    "### 2. Average Linkage (MEAN)\n",
    "\n",
    "This method provides a balance between the other two. It defines the distance from a city to the new cluster as the **average** of the distances to all of its members.\n",
    "\n",
    "Using our example:\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = (Distance(Tehran, IS) + Distance(Tehran, YA)) / 2`\n",
    "\n",
    "`Distance(Tehran, (IS-YA)) = (450 + 620) / 2 = 535`\n",
    "\n",
    "*   **Impact:** This method is less affected by outliers than Single Linkage and often provides a good, balanced result.\n",
    "\n",
    "---\n",
    "\n",
    "> \"This process of finding the closest pair and merging them is repeated until only one single cluster, containing all the cities, remains. The entire history of these merges creates a tree-like structure that we can then analyze.\"\n",
    "\n",
    "---\n",
    "\n",
    "This algorithm is quite heavy and may take long runtimes as we are doing the same thing over and over again on each node. so this might cost a fortune of resources on a very large dataset! But from the other hand, the answer is always the same. No need to double check.\n",
    "\n",
    "Enough **theory**! Let's dive into a real world code to see what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11490a745feb965",
   "metadata": {},
   "source": [
    "Imagine that an automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n",
    "\n",
    "Our objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46e52745cb6dd2",
   "metadata": {},
   "source": [
    "### Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea4c56ba781ddd4c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'data/cars.csv'\n",
    "\n",
    "pdf = pd.read_csv(filename)\n",
    "print (\"Shape of dataset: \", pdf.shape)\n",
    "\n",
    "pdf.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3a4f489234b84366",
   "metadata": {},
   "source": [
    "The feature sets include  price in thousands (price), engine size (engine_s), horsepower (horsepow), wheelbase (wheelbas), width (width), length (length), curb weight (curb_wgt), fuel capacity (fuel_cap) and fuel efficiency (mpg)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7db1fbecc24df",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Let's clean the dataset by dropping the rows that have null value:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e74850ed9e219073",
   "metadata": {},
   "source": [
    "print (\"Shape of dataset before cleaning: \", pdf.size)\n",
    "pdf[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']] = pdf[['sales', 'resale', 'type', 'price', 'engine_s',\n",
    "       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n",
    "       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\n",
    "pdf = pdf.dropna()\n",
    "pdf = pdf.reset_index(drop=True)\n",
    "print (\"Shape of dataset after cleaning: \", pdf.size)\n",
    "pdf.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d19ba83da81baba9",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Let's select our feature set:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9228fad49f198ca7",
   "metadata": {},
   "source": [
    "featureset = pdf[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cc22a10d6050dab",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Now we can normalize the feature set. **MinMaxScaler** transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "id": "e425c07157f75c5d",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = featureset.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "feature_mtx = min_max_scaler.fit_transform(x)\n",
    "feature_mtx [0:5]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist_matrix = euclidean_distances(feature_mtx,feature_mtx)\n",
    "print(dist_matrix)"
   ],
   "id": "ace551762be09b5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')"
   ],
   "id": "91b51f01046d6d08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pylab\n",
    "\n",
    "fig = pylab.figure(figsize=(18,50))\n",
    "def llf(id):\n",
    "    return '[%s %s %s]' % (pdf['manufact'][id], pdf['model'][id], int(float(pdf['type'][id])) )\n",
    "\n",
    "dendro = hierarchy.dendrogram(Z_using_dist_matrix,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')"
   ],
   "id": "f4da6640c5cad2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the next step, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset.\n",
    "\n",
    "*   Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "*   Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "*   Average linkage minimizes the average of the distances between all observations of pairs of clusters.\n"
   ],
   "id": "3b1157a3a8b5c9e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\n",
    "agglom.fit(dist_matrix)\n",
    "\n",
    "agglom.labels_"
   ],
   "id": "908ba2c82a76106f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pdf['cluster_'] = agglom.labels_\n",
    "pdf.head()"
   ],
   "id": "b8d55870f493e26d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pdf.groupby(['cluster_','type'])['cluster_'].count()",
   "id": "bbcdd2a902bd54df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agg_cars = pdf.groupby(['cluster_','type'])[['horsepow','engine_s','mpg','price']].mean()\n",
    "agg_cars"
   ],
   "id": "bf53ad171679bd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "for color, label in zip(colors, cluster_labels):\n",
    "    subset = agg_cars.loc[(label,),]\n",
    "    for i in subset.index:\n",
    "        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n",
    "    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\n",
    "plt.legend()\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('horsepow')\n",
    "plt.ylabel('mpg')"
   ],
   "id": "1e78bd5c8ee35919",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That was all about hierarchical. now let's dive into DBSCAN method",
   "id": "9d7ed44425cae3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DBSCAN Algorithm\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful clustering algorithm where clusters can be any shape, not just round, and their sizes can be different.\n",
    "\n",
    "You see, in an algorithm like K-Means, every single datapoint is forced into a cluster, no matter how far away or out of place it is. DBSCAN thinks differently. It believes that for a point to be part of a cluster, it must be in a \"dense\" or crowded neighborhood.\n",
    "\n",
    "DBSCAN classifies every point into one of three types:\n",
    "\n",
    "*   **Core Point:** A point in the heart of a dense cluster.\n",
    "*   **Border Point:** A point on the edge of a dense cluster.\n",
    "*   **Outlier (Noise):** A point that doesn't belong to any dense cluster.\n",
    "\n",
    "---\n",
    "\n",
    " ![](assets/core_border_outlier.png)\n",
    "\n",
    "---\n",
    "\n",
    "To decide which type each point is, DBSCAN uses two simple parameters:\n",
    "\n",
    "*   **`epsilon` (ε):** A distance or radius. This defines the \"neighborhood\" around each point.\n",
    "*   **`min_points`:** The minimum number of points required to form a dense region.\n",
    "\n",
    "Now, we can define the rules for classifying each point:\n",
    "\n",
    "*   A point is a **Core Point** if it has at least `min_points` neighbors (including itself) within its `epsilon` radius.\n",
    "*   A point is a **Border Point** if it has fewer than `min_points` neighbors, but it is close enough to be a neighbor of a **Core Point**.\n",
    "*   If a point is neither a Core nor a Border point, it is considered an **Outlier (Noise)**.\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/recognizing_outliers_borders_cores.png)\n",
    "\n",
    "---\n",
    "\n",
    "Finally, a cluster is formed by connecting Core points that are neighbors of each other. Any Border point that is a neighbor of one of these Core points becomes part of that same cluster. Outliers, belonging to no cluster, are left alone. no clusterings for them!\n",
    "\n",
    "Enough theory! Let's dive into the code to see it better."
   ],
   "id": "ea098bcde58531fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Import Libraries and Generate Data\n",
    "\n",
    "First, we'll import the necessary tools and create our dataset. We'll generate 200 data points shaped like two interleaving crescent moons and add some random noise to represent outliers."
   ],
   "id": "5fbb695486b0d61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# n_samples: The total number of points to generate.\n",
    "# noise: The standard deviation of Gaussian noise added to the data.\n",
    "# random_state: Ensures we get the same 'random' points every time we run the code.\n",
    "X, y = make_moons(n_samples=1000, noise=0.09)\n",
    "\n",
    "df = pd.DataFrame(X, columns=['feature1', 'feature2'])\n",
    "\n",
    "df.head()"
   ],
   "id": "14a4b3571f58e9d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Visualize the Raw Data\n",
    "\n",
    "Before we do any clustering, let's look at our data. This helps us understand the challenge. You can clearly see the two \"moon\" shapes that a simple circular algorithm like K-Means would struggle with."
   ],
   "id": "50b75d364b27fcf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['feature1'], df['feature2'], c='gray')\n",
    "plt.title('Raw, Unclustered \"Moons\" Data', fontsize=16)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ],
   "id": "72608509dc399a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Set Up and Run the DBSCAN Algorithm\n",
    "\n",
    "Now, we create an instance of the `DBSCAN` model and fit it to our data. The most important part is choosing the `eps` (epsilon) and `min_samples` parameters. Finding the best values often requires some experimentation.\n",
    "\n",
    "*   `eps=0.2`: We'll define the neighborhood radius as 0.2.\n",
    "*   `min_samples=5`: We'll say a point needs at least 5 neighbors to be a Core point."
   ],
   "id": "aa15d20deace9e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "\n",
    "# Fit the model to our data\n",
    "dbscan.fit(df)"
   ],
   "id": "fd5c78a7b1d7198c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Analyze the Clustering Results\n",
    "\n",
    "After fitting, the results are stored in the model. The most important attribute is `labels_`, where each point is assigned a cluster ID. DBSCAN uses **-1** to label all the points it considers **outliers (noise)**."
   ],
   "id": "8a77d9fa3ab62847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = dbscan.labels_\n",
    "df['cluster'] = labels\n",
    "\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "print(f\"Estimated number of noise points: {n_noise_}\")\n",
    "\n",
    "df.head()"
   ],
   "id": "70e515fc1d8be533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Visualize the Final DBSCAN Clusters\n",
    "\n",
    "This is the final and most revealing step. We'll plot the data again, but this time we will color the points according to the cluster they belong to. We will make the noise points small and black to show that they don't belong to any group.\n",
    "\n",
    "We can also highlight the **Core points** to see the \"hearts\" of the clusters that DBSCAN identified."
   ],
   "id": "3c421f129d340507"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "unique_labels = set(labels)\n",
    "\n",
    "# Define colors for the clusters\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=8)\n",
    "\n",
    "plt.title(f'DBSCAN Clustering | Clusters: {n_clusters_}', fontsize=16)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ],
   "id": "799dafc5489a4a91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "\n",
    "This report provided a comprehensive introduction to the principles and practices of clustering, a fundamental task in unsupervised machine learning. The primary objective of clustering—to group data by maximizing inter-cluster distance and minimizing intra-cluster distance—was established as the core concept.\n",
    "\n",
    "We explored three foundational algorithms, each representing a different approach to identifying structure in unlabeled data:\n",
    "\n",
    "1.  **K-Means:** A fast, centroid-based method ideal for identifying simple, spherical clusters.\n",
    "2.  **Hierarchical Clustering:** A connectivity-based method that builds a complete hierarchy of clusters, visualized through a dendrogram.\n",
    "3.  **DBSCAN:** A density-based method with the unique ability to find arbitrarily shaped clusters and effectively identify noise and outliers.\n",
    "\n",
    "\n",
    "---\n",
    "### Acknowledgements\n",
    "\n",
    "I would like to extend my gratitude to **_jadi mirmirani_**, the Tutor of **_Maktabkooneh_** who helped me understand the fundumentals of **_clustering_**.\n",
    "\n",
    "A huge thanks to Google's Gemini for its assistance in structuring the content, clarifying concepts and providing feedback throughout the preparation of this report.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "The theoretical concepts and practical code examples in this report were primarily adapted from the following educational resources:\n",
    "\n",
    "1.  Mirmirani, J. (n.d.). [*آموزش یادگیری ماشین با پایتون (Machine Learning with Python)*](https://maktabkhooneh.org/course/یادگیری-ماشین-پایتون-mk1318) [Online Course]. Maktabkhooneh.\n",
    "2.  Mirmirani, J. (n.d.). [*machine_learning_with_python_jadi*](https://github.com/jadijadi/machine_learning_with_python_jadi) [Source Code Repository]. GitHub.\n",
    "\n",
    "---\n",
    "**Prepared for:**\n",
    "\n",
    "Dr. Ali Amiri\n",
    "\n",
    "**Prepared by:**\n",
    "\n",
    "Pouya Baniadam\n",
    "\n",
    "(Master Software Engineering at Zanjan university)\n",
    "\n",
    "**Course:**\n",
    "\n",
    "**Machine learning**\n",
    "\n",
    "**Date:**\n",
    "\n",
    "November 1, 2025"
   ],
   "id": "cab04fa8029ea6f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "74b2995d79f6afed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
